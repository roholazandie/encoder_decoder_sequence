## Attention vs Non-Attention models

The vanila lstm approach to predict the output sequence y from X is not enough
because it has a fixed length vector for representation the whole sequence
we need an attention mechanism that tells the LSTM where to attend